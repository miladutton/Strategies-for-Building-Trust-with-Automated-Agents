# Strategies for Building Trust with Automated Agents

This project explores how different conversational styles (friendly, moderate, and professional) used by AI voice agents affect user trust and experience.

We studied this by having participants interact with different agents and analyzed their responses using both quantitative (ANOVA, post-hoc tests) and qualitative (sentiment and thematic coding) methods.

## ğŸ“ What's in this repo

data/ â€“ Raw and cleaned survey response data
code/ â€“ R scripts for analysis (e.g., sentiment, ANOVA)
final_report/ â€“ Full PDF report with all results, visuals, and appendices
README.md â€“ This file

## ğŸ” What We Found

Friendly Agent had the highest engagement and emotional connection scores
Engagement scores were generally higher than emotional connection across all agents
Sentiment analysis showed more positive responses for the Friendly Agent
Qualitative coding showed themes of enjoyment and natural flow with the Friendly Agent

## ğŸ“„ Report

See the full PDF report here. It includes:

Research background
Methods and analysis
Visuals and tables
Appendices (agent prompts, data cleaning, etc.)
